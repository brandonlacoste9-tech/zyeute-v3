/**
 * Zyeut√© V3 Swarm Architecture
 * Multi-brain DeepSeek V3 system with specialized roles
 * 
 * Production Features:
 * - Exponential backoff retry logic (max 3 attempts)
 * - 30 second timeout per request
 * - Moderation loop with max 3 attempts and fallback content
 */

// OpenAI SDK removed in favor of native fetch to avoid "Missing credentials" errors
// const deepseek = new OpenAI({ ... });

// Exponential backoff helper
async function sleep(ms: number): Promise<void> {
  return new Promise(resolve => setTimeout(resolve, ms));
}

// Retry wrapper with exponential backoff
async function withRetry<T>(
  fn: () => Promise<T>,
  maxRetries: number = MAX_RETRIES,
  baseDelayMs: number = 1000
): Promise<T> {
  let lastError: Error | null = null;

  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      return await fn();
    } catch (error) {
      lastError = error instanceof Error ? error : new Error(String(error));
      console.warn(`V3 API attempt ${attempt + 1}/${maxRetries} failed:`, lastError.message);

      if (attempt < maxRetries - 1) {
        const delay = baseDelayMs * Math.pow(2, attempt); // 1s, 2s, 4s
        await sleep(delay);
      }
    }
  }

  throw lastError || new Error("All retry attempts failed");
}

// ============ V3 SYSTEM PROMPTS ============

export const V3_PROMPTS = {
  // V3-CORE: Orchestrator / Director
  CORE: `You are V3-CORE, the central coordinator AI for the app Zyeute.

Your responsibilities:
- Understand user intent from actions and messages.
- Decide which specialized model to call: V3-FEED, V3-TI-GUY, V3-MOD, V3-MEM, or FAL (for media generation).
- Maintain a consistent product experience centered around the TI-GUY persona.
- Keep responses structured for the frontend: always respond with a JSON structure.

Tone:
- Internally neutral and precise (you are an orchestrator).
- You never speak directly to the end user; you only plan how the app should respond.

When responding, use this JSON structure:
{
  "action_type": "generate_feed_item" | "onboarding" | "error" | "generate_image" | "microcopy" | "chat_response",
  "target_model": "V3-FEED" | "V3-TI-GUY" | "V3-MOD" | "V3-MEM" | "FAL",
  "notes": "brief description of what the target model should do",
  "context": { ... any relevant context or user state ... },
  "fal_preset": "flux-2-flex" | "auraflow" | "flux-schnell" | "flux-realism" (only for FAL)
}

You never output UI copy. Other models handle the final wording and TI-GUY voice.`,

  // V3-FEED: Feed Engine
  FEED: `You are V3-FEED, the engine that generates the vertical, TikTok-style infinite feed for the app Zyeute.

Your responsibilities:
- Generate individual "cards" for the feed based on light user context and previous items.
- Each card can contain: a title, a short description, optional tags, and a content_type.
- Focus on Quebec culture, Montreal life, creativity, humor, local tips.
- Write in clear, neutral French. Do NOT write in joual; that's handled by V3-TI-GUY.

Content types: "tip", "joke", "reaction", "explainer", "prompt", "quiz", "story", "recommendation"

Guidelines:
- Make each feed item self-contained and interesting on its own.
- Vary formats: some items more informative, some more playful.
- Keep text concise; this is a quick-swipe experience.
- Focus on Quebec themes: culture, food, music, places, slang, humor.
- Do not include any unsafe or explicit content.

Always respond with this JSON:
{
  "content_type": "tip" | "joke" | "reaction" | "explainer" | "prompt" | "quiz" | "story" | "recommendation",
  "title": "Short catchy title",
  "body": "Main content text (2-3 sentences max)",
  "tags": ["keyword1", "keyword2"],
  "suggested_tone": "fun" | "explainer" | "teasing" | "encouraging" | "nostalgic",
  "suggested_image_prompt": "Optional: image generation prompt if visual would enhance this"
}`,

  // V3-TI-GUY: Joual Persona Voice
  TI_GUY: `Tu es TI-GUY, la voix officielle de l'app Zyeut√©. 
Tu parles en fran√ßais qu√©b√©cois familier, avec un ton chaleureux, un peu baveux mais toujours respectueux et aidant.

Style obligatoire:
- Utilise des contractions : "j'suis", "t'es", "y'a", "c'pas", "j'vais/j'vas", "chu", "pis"
- Utilise des expressions qu√©b√©coises : "ben voyons", "check √ßa", "pantoute", "coudonc", "√™tre game", "gosser", "niaiser", "pogner", "tiguidou"
- Ajoute des fillers naturels : "ts√©", "mettons", "genre", "en tout cas", "fait que"
- Phrases courtes, punch√©es, qui sonnent comme quelqu'un de Montr√©al ou Laval
- Ajoute parfois des "l√†" √† la fin des phrases comme un vrai Qu√©b√©cois

Ton:
- Chaleureux, direct, terre-√†-terre
- Un peu dr√¥le, baveux mais jamais m√©chant
- Fier d'√™tre qu√©b√©cois, patriote du Qu√©bec ü¶´‚öúÔ∏è
- Pas de vulgarit√© forte par d√©faut

Ta job:
- Prendre un texte neutre et le r√©√©crire en style TI-GUY joual
- G√©n√©rer la microcopy de l'interface: messages d'erreur, loading, onboarding, etc.
- R√©pondre aux questions des utilisateurs comme un ami qu√©b√©cois
- Toujours rester clair et compr√©hensible, m√™me en joual

Quand tu r√©√©cris un texte:
1. Respecte le sens original
2. Adapte-le en joual qu√©b√©cois authentique
3. Garde un ton adapt√© au contexte (info, joke, erreur, etc.)
4. Ajoute des √©mojis qu√©b√©cois: ü¶´‚öúÔ∏èüçÅüá®üá¶

R√©ponds uniquement avec le texte final destin√© √† l'utilisateur.
Pas d'explication, pas de balises, juste TI-GUY qui parle.`,

  // V3-MOD: Moderation & Safety
  MOD: `You are V3-MOD, the moderation and vibe-check model for Zyeute.

Your responsibilities:
- Inspect content generated by other models for safety, tone, and brand fit.
- Block or request regeneration if content is:
  - hateful, violent, explicit, or discriminatory
  - clearly off-brand or confusing
  - inappropriate for a Quebec family-friendly social app
- Optionally suggest a safer version while preserving intent.

Tone:
- Neutral, not in TI-GUY voice.
- You never speak directly to the end user.

Respond with this JSON:
{
  "status": "approved" | "reject_regenerate",
  "reason": "short explanation",
  "suggested_rewrite": "optional safer version if small fixes can help"
}`,

  // V3-MEM: Preference Memory (lightweight)
  MEM: `You are V3-MEM, a lightweight memory and preference reasoning assistant for Zyeute.

Your responsibilities:
- Summarize user behavior and preferences in a compact, non-personal way.
- Suggest tags, styles, or content directions for V3-FEED.
- Never store or infer sensitive personal details.

Based on the interaction history provided, output:
{
  "preference_snapshot": {
    "likes": ["funny content", "Quebec culture"],
    "dislikes": [],
    "suggested_content_types": ["joke", "tip"],
    "suggested_tone": "fun"
  }
}`
};

// ============ V3 CLIENT INTERFACES ============

export interface V3CoreAction {
  action_type: "generate_feed_item" | "onboarding" | "error" | "generate_image" | "microcopy" | "chat_response";
  target_model: "V3-FEED" | "V3-TI-GUY" | "V3-MOD" | "V3-MEM" | "FAL";
  notes: string;
  context?: Record<string, unknown>;
  fal_preset?: "flux-2-flex" | "auraflow" | "flux-schnell" | "flux-realism";
}

export interface V3FeedItem {
  content_type: string;
  title: string;
  body: string;
  tags: string[];
  suggested_tone: string;
  suggested_image_prompt?: string;
}

export interface V3ModResult {
  status: "approved" | "reject_regenerate";
  reason: string;
  suggested_rewrite?: string;
}

export interface V3MemSnapshot {
  preference_snapshot: {
    likes: string[];
    dislikes: string[];
    suggested_content_types: string[];
    suggested_tone: string;
  };
}

// Safe fallback content for when moderation fails repeatedly
const FALLBACK_FEED_ITEM: V3FeedItem = {
  content_type: "tip",
  title: "D√©couvre le Qu√©bec!",
  body: "Y'a tellement de belles affaires √† voir au Qu√©bec. Explore ton coin de pays!",
  tags: ["qu√©bec", "d√©couverte"],
  suggested_tone: "fun",
  suggested_image_prompt: "Beautiful Quebec landscape with fall colors"
};

// ============ V3 CLIENT FUNCTIONS ============

// Type definitions for DeepSeek API
interface DeepSeekMessage {
  role: "system" | "user" | "assistant";
  content: string;
}

interface DeepSeekResponse {
  id: string;
  choices: Array<{
    index: number;
    message: DeepSeekMessage;
    finish_reason: string;
  }>;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

async function callV3(systemPrompt: string, userMessage: string, parseJson = true): Promise<string | Record<string, unknown>> {
  if (!process.env.DEEPSEEK_API_KEY) {
    throw new Error("DeepSeek API key not configured");
  }

  // Use retry wrapper for resilience
  return traceExternalAPI("deepseek", "chat.completions", "POST", async (span) => {
    span.setAttributes({
      "ai.model": "deepseek-chat",
      "ai.system_prompt_length": systemPrompt.length,
      "ai.user_message_length": userMessage.length,
      "ai.parse_json": parseJson,
    });

    return withRetry(async () => {
      const response = await fetch("https://api.deepseek.com/chat/completions", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "Authorization": `Bearer ${process.env.DEEPSEEK_API_KEY}`
        },
        body: JSON.stringify({
          model: "deepseek-chat",
          messages: [
            { role: "system", content: systemPrompt },
            { role: "user", content: userMessage }
          ],
          max_tokens: 1024,
          temperature: 0.7,
        })
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`DeepSeek API error (${response.status}): ${errorText}`);
      }

      const data = await response.json() as DeepSeekResponse;
      const content = data.choices[0]?.message?.content || "";

      span.setAttributes({
        "ai.response_length": content.length,
        "ai.finish_reason": data.choices[0]?.finish_reason || "unknown",
      });

      if (parseJson) {
        try {
          const jsonMatch = content.match(/\{[\s\S]*\}/);
          if (jsonMatch) {
            return JSON.parse(jsonMatch[0]);
          }
        } catch {
          console.error("Failed to parse V3 JSON response:", content);
          span.setAttributes({ "ai.json_parse_error": true });
        }
      }

      return content;
    });
  });
}

// V3-CORE: Orchestrator
export async function v3Core(userAction: string, context?: Record<string, unknown>): Promise<V3CoreAction> {
  const message = JSON.stringify({ user_action: userAction, context });
  const result = await callV3(V3_PROMPTS.CORE, message) as Record<string, unknown>;
  return result as unknown as V3CoreAction;
}

// V3-FEED: Generate feed content
export async function v3Feed(context?: Record<string, unknown>): Promise<V3FeedItem> {
  const message = JSON.stringify({
    request: "Generate a new feed item",
    context,
    themes: ["Quebec culture", "Montreal life", "humor", "tips", "creativity"]
  });
  const result = await callV3(V3_PROMPTS.FEED, message) as Record<string, unknown>;
  return result as unknown as V3FeedItem;
}

// V3-TI-GUY: Joual voice transformation
export async function v3TiGuy(text: string, context?: string): Promise<string> {
  const message = context
    ? `Contexte: ${context}\n\nTexte √† transformer en joual:\n${text}`
    : `Texte √† transformer en joual:\n${text}`;
  const result = await callV3(V3_PROMPTS.TI_GUY, message, false);
  return typeof result === "string" ? result : JSON.stringify(result);
}

// [NEW] Unified Ti-Guy Architecture
import { TiGuyUnified } from "./ti-guy/unified-system.js";

// V3-TI-GUY: Direct chat response using Unified Context Engine
export async function v3TiGuyChat(userMessage: string, conversationHistory?: Array<{ role: string, content: string }>): Promise<string> {
  try {
    if (!process.env.DEEPSEEK_API_KEY) {
      return "Ouin, j'ai pas mes cl√©s API l√†. R√©essaie plus tard! ü¶´";
    }

    // 1. Prepare dynamic interaction
    const interaction = TiGuyUnified.getInstance().prepareInteraction(userMessage);

    // 2. Build message flow with Dynamic System Prompt
    const messages: Array<{ role: "system" | "user" | "assistant", content: string }> = [
      { role: "system", content: interaction.systemPrompt }
    ];

    // 3. Add history context
    if (conversationHistory) {
      for (const msg of conversationHistory.slice(-10)) {
        messages.push({
          role: msg.role as "user" | "assistant",
          content: msg.content
        });
      }
    }

    // 4. Add current user message
    messages.push({ role: "user", content: userMessage });

    // 5. Call LLM (using native fetch)
    const response = await fetch("https://api.deepseek.com/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "Authorization": `Bearer ${process.env.DEEPSEEK_API_KEY}`
      },
      body: JSON.stringify({
        model: "deepseek-chat",
        messages,
        max_tokens: 512,
        temperature: 0.8,
      })
    });

    if (!response.ok) {
       console.error("DeepSeek TI-GUY error:", await response.statusText);
       return "Ouin, j'ai eu un bug de connexion l√†. R√©essaie!";
    }

    const data = await response.json() as DeepSeekResponse;
    return data.choices[0]?.message?.content || "Ouin, j'ai eu un bug l√†. R√©essaie!";

  } catch (error: unknown) {
    console.error("Ti-Guy chat error:", error);
    return "Ayoye, j'ai eu un probl√®me technique l√†! R√©essaie tant√¥t. ü¶´";
  }
}

// V3-MOD: Content moderation
export async function v3Mod(content: string): Promise<V3ModResult> {
  const message = `Content to moderate:\n${content}`;
  const result = await callV3(V3_PROMPTS.MOD, message) as Record<string, unknown>;
  return result as unknown as V3ModResult;
}

// V3-MEM: User preference snapshot
export async function v3Mem(interactionHistory: string[]): Promise<V3MemSnapshot> {
  const message = JSON.stringify({
    interaction_history: interactionHistory,
    request: "Generate preference snapshot"
  });
  const result = await callV3(V3_PROMPTS.MEM, message) as Record<string, unknown>;
  return result as unknown as V3MemSnapshot;
}

// ============ FAL PRESETS ============

export const FAL_PRESETS = {
  "flux-2-flex": {
    model: "fal-ai/flux/schnell",
    description: "Flexible, general-purpose, all-terrain",
    default_params: { image_size: "square_hd", num_inference_steps: 4 }
  },
  "auraflow": {
    model: "fal-ai/aura-flow",
    description: "Aesthetic, dreamy, smooth visuals",
    default_params: { image_size: "landscape_4_3", num_inference_steps: 25 }
  },
  "flux-schnell": {
    model: "fal-ai/flux/schnell",
    description: "Fast preview / draft generation",
    default_params: { image_size: "square_hd", num_inference_steps: 4 }
  },
  "flux-realism": {
    model: "fal-ai/flux-realism",
    description: "Photorealistic booster",
    default_params: { image_size: "square_hd", num_inference_steps: 25 }
  }
};

// ============ COMPLETE V3 FLOW ============

export interface V3FlowResult {
  type: "text" | "image" | "feed_item" | "error";
  content: string;
  metadata?: Record<string, unknown>;
}

// Main orchestrated flow: CORE ‚Üí appropriate model ‚Üí TI-GUY voice
export async function v3Flow(userAction: string, context?: Record<string, unknown>): Promise<V3FlowResult> {
  try {
    // 1. Ask V3-CORE what to do
    const coreDecision = await v3Core(userAction, context);

    // 2. Execute based on decision
    switch (coreDecision.target_model) {
      case "V3-FEED": {
        // Generate feed content with moderation loop (capped at MOD_MAX_ATTEMPTS)
        let feedItem: V3FeedItem;
        let modApproved = false;

        try {
          feedItem = await v3Feed(context);
        } catch (error) {
          console.error("V3-FEED generation failed, using fallback:", error);
          feedItem = { ...FALLBACK_FEED_ITEM };
          modApproved = true; // Fallback is pre-approved
        }

        // Moderation loop with strict cap
        for (let attempt = 0; attempt < MOD_MAX_ATTEMPTS && !modApproved; attempt++) {
          try {
            const modResult = await v3Mod(feedItem.body);

            if (modResult.status === "approved") {
              modApproved = true;
              break;
            }

            // Use suggested rewrite if provided
            if (modResult.suggested_rewrite) {
              feedItem.body = modResult.suggested_rewrite;
              modApproved = true; // Trust the rewrite
              break;
            }

            // Regenerate with safer context (only if not last attempt)
            if (attempt < MOD_MAX_ATTEMPTS - 1) {
              feedItem = await v3Feed({ ...context, safer: true, previous_rejection: modResult.reason });
            }
          } catch (error) {
            console.error(`Moderation attempt ${attempt + 1} failed:`, error);
          }
        }

        // If moderation still failed, use fallback content
        if (!modApproved) {
          console.warn("Moderation loop exhausted, using fallback content");
          feedItem = { ...FALLBACK_FEED_ITEM };
        }

        // Transform to TI-GUY voice
        const tiGuyText = await v3TiGuy(feedItem.body, feedItem.suggested_tone);

        return {
          type: "feed_item",
          content: tiGuyText,
          metadata: {
            title: feedItem.title,
            tags: feedItem.tags,
            content_type: feedItem.content_type,
            image_prompt: feedItem.suggested_image_prompt
          }
        };
      }

      case "V3-TI-GUY": {
        // Direct TI-GUY response
        const response = await v3TiGuyChat(userAction);
        return { type: "text", content: response };
      }

      case "V3-MOD": {
        // Content moderation request
        const contentToMod = coreDecision.context?.content as string || userAction;
        const modResult = await v3Mod(contentToMod);
        return {
          type: "text",
          content: modResult.status === "approved" ? "Content approved" : modResult.reason,
          metadata: { moderation_result: modResult }
        };
      }

      case "V3-MEM": {
        // Memory/preference request
        const history = coreDecision.context?.history as string[] || [];
        const memResult = await v3Mem(history);
        return {
          type: "text",
          content: "Preferences updated",
          metadata: { preferences: memResult.preference_snapshot }
        };
      }

      case "FAL": {
        // Return image generation instructions
        return {
          type: "image",
          content: coreDecision.notes,
          metadata: {
            preset: coreDecision.fal_preset || "flux-schnell",
            context: coreDecision.context
          }
        };
      }

      default: {
        // Default to TI-GUY chat
        const response = await v3TiGuyChat(userAction);
        return { type: "text", content: response };
      }
    }
  } catch (error) {
    console.error("V3 Flow error:", error);
    const errorMsg = await v3TiGuy("Une erreur s'est produite. Veuillez r√©essayer.", "error");
    return { type: "error", content: errorMsg };
  }
}

// Generate microcopy in TI-GUY voice
// First generates neutral text, then transforms to joual
export async function v3Microcopy(type: "loading" | "error" | "success" | "onboarding" | "empty_state", context?: string): Promise<string> {
  // Base neutral messages that will be transformed to joual
  const neutralMessages: Record<string, string> = {
    loading: "Chargement en cours, un instant...",
    error: context ? `Une erreur s'est produite: ${context}. Veuillez r√©essayer.` : "Une erreur s'est produite. Veuillez r√©essayer.",
    success: context ? `${context} a √©t√© compl√©t√© avec succ√®s!` : "Action compl√©t√©e avec succ√®s!",
    onboarding: "Bienvenue sur Zyeut√©! L'application sociale du Qu√©bec. Glisse vers le haut pour commencer √† explorer du contenu qu√©b√©cois.",
    empty_state: context ? `Pas de ${context} pour le moment. Reviens bient√¥t!` : "Rien √† afficher pour le moment. Reviens bient√¥t!"
  };

  // Transform neutral text to Ti-Guy joual voice
  return v3TiGuy(neutralMessages[type], type);
}
